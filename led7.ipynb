{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TRAIN_DATA_FILE = \"train_data.csv\"\n",
    "TEST_DATA_FILE = \"test_data.csv\"\n",
    "ALL_CASES_INPUT_DATA = \"all_cases_input_data.csv\"\n",
    "ALL_CASES_PREDICTIONS = \"all_cases_predictions.csv\"\n",
    "ALL_CASES_PREDICTIONS_COLUMS = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "def load_data(file=TRAIN_DATA_FILE, header=True):\n",
    "    csv_path = os.path.join(\"\", file)\n",
    "    if header:\n",
    "        return pd.read_csv(csv_path)\n",
    "    else:\n",
    "        return pd.read_csv(csv_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(TRAIN_DATA_FILE)\n",
    "train_labels = train_data[\"DIGIT\"]\n",
    "train_data.drop(\"DIGIT\", axis=1, inplace=True)\n",
    "\n",
    "test_data = load_data(TEST_DATA_FILE)\n",
    "test_labels = test_data[\"DIGIT\"]\n",
    "test_data.drop(\"DIGIT\", axis=1, inplace=True)\n",
    "\n",
    "all_cases_input_data = load_data(ALL_CASES_INPUT_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_dims_variances(x, minDim, tol=None, thres=0.01):\n",
    "    dims = []\n",
    "    variances = []\n",
    "    optimum_dim = minDim\n",
    "    prev_min_variance = None\n",
    "    dim = minDim\n",
    "    \n",
    "    while(True):\n",
    "        pca = PCA(n_components=dim)\n",
    "        pca.fit(x)\n",
    "        variance = np.array(pca.explained_variance_ratio_)\n",
    "        min_variance = variance.min()\n",
    "        \n",
    "        dims.append(dim)\n",
    "        variances.append(min_variance)\n",
    "        \n",
    "        if tol != None and prev_min_variance != None and min_variance + tol > prev_min_variance:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            if prev_min_variance != None and min_variance < thres:\n",
    "                break\n",
    "                \n",
    "        prev_min_variance = min_variance\n",
    "        optimum_dim = dim\n",
    "        dim = dim + 1\n",
    "\n",
    "    return dims, variances, optimum_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def process_data(x, y=None, poly_features=None, pca=None, OPTIMUM_DIMENSION=None):\n",
    "    training_features = x.copy()\n",
    "    testing_features = y.copy()\n",
    "    \n",
    "    if poly_features == None:\n",
    "        poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        poly_features.fit(training_features)\n",
    "        \n",
    "    training_features = poly_features.transform(training_features)\n",
    "    testing_features = poly_features.transform(testing_features)\n",
    "\n",
    "    if OPTIMUM_DIMENSION == None:\n",
    "        dims, variances, OPTIMUM_DIMENSION = get_dims_variances(x=training_features, minDim=2, thres=0.01)\n",
    "        print(\"Optimum Dimensions: \", OPTIMUM_DIMENSION)\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(dims, variances)\n",
    "        plt.show()\n",
    "        dim_df = pd.DataFrame()\n",
    "        dim_df[\"DIM\"] = dims\n",
    "        dim_df[\"VAR\"] = variances\n",
    "        print(dim_df)\n",
    "\n",
    "    if pca == None:  \n",
    "        pca = PCA(random_state=42, n_components=OPTIMUM_DIMENSION)\n",
    "        pca.fit(training_features)\n",
    "        \n",
    "    training_features = pca.transform(training_features)\n",
    "    testing_features = pca.transform(testing_features)\n",
    "    \n",
    "    return training_features, testing_features, poly_features, pca, OPTIMUM_DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Dimensions:  18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103a54710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DIM       VAR\n",
      "0     2  0.126065\n",
      "1     3  0.091955\n",
      "2     4  0.078825\n",
      "3     5  0.072726\n",
      "4     6  0.062582\n",
      "5     7  0.052270\n",
      "6     8  0.047362\n",
      "7     9  0.044440\n",
      "8    10  0.042660\n",
      "9    11  0.036638\n",
      "10   12  0.030894\n",
      "11   13  0.025990\n",
      "12   14  0.022621\n",
      "13   15  0.016864\n",
      "14   16  0.013965\n",
      "15   17  0.011354\n",
      "16   18  0.010331\n",
      "17   19  0.006118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "training_features = imputer.fit_transform(train_data)\n",
    "testing_features = imputer.transform(test_data)\n",
    "all_cases_features = imputer.transform(all_cases_input_data)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "training_features = scalar.fit_transform(training_features)\n",
    "testing_features = scalar.transform(testing_features)\n",
    "all_cases_features = scalar.transform(all_cases_features)\n",
    "\n",
    "training_features, testing_features, poly_features, pca, OPTIMUM_DIMENSION = process_data(x=training_features, y=testing_features)\n",
    "\n",
    "training_labels = train_labels.values\n",
    "testing_labels = test_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Scores on training set\n",
      " [0.6 0.4]\n",
      "\n",
      "\n",
      "Accuracy on testing data set\n",
      " 0.9\n"
     ]
    }
   ],
   "source": [
    "# SGD Classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import clone\n",
    "\n",
    "X_train = training_features\n",
    "Y_train = training_labels\n",
    "X_test = testing_features\n",
    "Y_test = testing_labels\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42, penalty=\"l1\")\n",
    "cross_val_scores = cross_val_score(clone(sgd_clf), X_train, Y_train, cv=2, scoring=\"accuracy\")\n",
    "print(\"Cross Val Scores on training set\\n\", cross_val_scores)\n",
    "\n",
    "sgd_clf.fit(X_train, Y_train)\n",
    "print(\"\\n\\nAccuracy on testing data set\\n\", sum(Y_test == sgd_clf.predict(X_test)) / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params:  {'algorithm': 'auto', 'n_neighbors': 2, 'weights': 'distance'}\n",
      "\n",
      "Cross Val Scores on training set\n",
      " [0.5 0.5]\n",
      "\n",
      "\n",
      "Accuracy on testing data set\n",
      " 0.9\n"
     ]
    }
   ],
   "source": [
    "# KNeighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "X_train = training_features\n",
    "Y_train = training_labels\n",
    "X_test = testing_features\n",
    "Y_test = testing_labels\n",
    "\n",
    "parameters = {'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'n_neighbors' : [2,3,4,5,6,7,8,9,10],\n",
    "              'weights' : ['uniform', 'distance']\n",
    "             }\n",
    "clf = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"\\nBest params: \", clf.best_params_)\n",
    "\n",
    "knn_clf = KNeighborsClassifier(algorithm='auto', n_neighbors=2, weights='uniform')\n",
    "print(\"\\nCross Val Scores on training set\\n\", cross_val_score(clone(knn_clf), X_train, Y_train, cv=2, scoring=\"accuracy\"))\n",
    "\n",
    "knn_clf.fit(X_train, Y_train)\n",
    "print(\"\\n\\nAccuracy on testing data set\\n\", sum(Y_test == clf.predict(X_test)) / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Scores on training set\n",
      " [0.15 0.1 ]\n",
      "\n",
      "\n",
      "Accuracy on testing data set\n",
      " 0.95\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "X_train = training_features\n",
    "Y_train = training_labels\n",
    "X_test = testing_features\n",
    "Y_test = testing_labels\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42, oob_score=True, n_estimators=5)\n",
    "print(\"Cross Val Scores on training set\\n\", cross_val_score(clone(forest_clf), X_train, Y_train, cv=2, scoring=\"accuracy\"))\n",
    "\n",
    "forest_clf.fit(X_train, Y_train)\n",
    "print(\"\\n\\nAccuracy on testing data set\\n\", sum(Y_test == forest_clf.predict(X_test)) / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               4864      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 7,434\n",
      "Trainable params: 7,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30 samples, validate on 20 samples\n",
      "Epoch 1/15\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 2.7779 - acc: 0.0667 - val_loss: 2.5596 - val_acc: 0.0500\n",
      "Epoch 2/15\n",
      "30/30 [==============================] - 0s 710us/step - loss: 2.8183 - acc: 0.1667 - val_loss: 2.4088 - val_acc: 0.1000\n",
      "Epoch 3/15\n",
      "30/30 [==============================] - 0s 716us/step - loss: 2.8517 - acc: 0.0667 - val_loss: 2.2549 - val_acc: 0.2500\n",
      "Epoch 4/15\n",
      "30/30 [==============================] - 0s 825us/step - loss: 2.6449 - acc: 0.1667 - val_loss: 2.0989 - val_acc: 0.4500\n",
      "Epoch 5/15\n",
      "30/30 [==============================] - 0s 961us/step - loss: 2.2257 - acc: 0.1667 - val_loss: 1.9474 - val_acc: 0.5500\n",
      "Epoch 6/15\n",
      "30/30 [==============================] - 0s 721us/step - loss: 2.0006 - acc: 0.4333 - val_loss: 1.8031 - val_acc: 0.6000\n",
      "Epoch 7/15\n",
      "30/30 [==============================] - 0s 965us/step - loss: 1.7716 - acc: 0.3667 - val_loss: 1.6672 - val_acc: 0.6000\n",
      "Epoch 8/15\n",
      "30/30 [==============================] - 0s 937us/step - loss: 2.1012 - acc: 0.3667 - val_loss: 1.5402 - val_acc: 0.7500\n",
      "Epoch 9/15\n",
      "30/30 [==============================] - 0s 761us/step - loss: 1.7020 - acc: 0.4667 - val_loss: 1.4217 - val_acc: 0.8000\n",
      "Epoch 10/15\n",
      "30/30 [==============================] - 0s 985us/step - loss: 1.6855 - acc: 0.4333 - val_loss: 1.3116 - val_acc: 0.8500\n",
      "Epoch 11/15\n",
      "30/30 [==============================] - 0s 762us/step - loss: 1.6129 - acc: 0.5667 - val_loss: 1.2099 - val_acc: 0.9500\n",
      "Epoch 12/15\n",
      "30/30 [==============================] - 0s 965us/step - loss: 1.5617 - acc: 0.4667 - val_loss: 1.1233 - val_acc: 0.9500\n",
      "Epoch 13/15\n",
      "30/30 [==============================] - 0s 914us/step - loss: 1.3574 - acc: 0.5333 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "30/30 [==============================] - 0s 769us/step - loss: 1.3631 - acc: 0.5333 - val_loss: 0.9652 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "30/30 [==============================] - 0s 821us/step - loss: 1.2018 - acc: 0.7000 - val_loss: 0.8972 - val_acc: 1.0000\n",
      "20/20 [==============================] - 0s 52us/step\n",
      "Test loss: 0.8972346186637878\n",
      "Test accuracy: 1.0\n",
      "Test accuracy/loss ratio: 1.1145356846454009\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X_train = training_features\n",
    "Y_train = training_labels\n",
    "X_test = testing_features\n",
    "Y_test = testing_labels\n",
    "batch_size = 3\n",
    "num_classes = 10\n",
    "epochs = 15\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "binarizer.fit(Y_train)\n",
    "Y_train = binarizer.transform(Y_train)\n",
    "Y_test = binarizer.transform(Y_test)\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test accuracy/loss ratio:', score[1] / score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11031048 0.06208147 0.11678445 0.11123959 0.08184429 0.11866929\n",
      "  0.1073188  0.06332638 0.09520247 0.13322277]]\n",
      "[[0.  0.4 0.  0.  0.2 0.  0.  0.  0.2 0.2]]\n",
      "[[0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.5]]\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "x = [[0,0,0,1,0,0,0]]\n",
    "x_,_,_,_,_ = process_data(x=x, y=x, poly_features=poly_features, pca=pca, OPTIMUM_DIMENSION=OPTIMUM_DIMENSION)\n",
    "print(model.predict(x_.copy()))\n",
    "print(forest_clf.predict_proba(x_.copy()))\n",
    "print(knn_clf.predict_proba(x_.copy()))\n",
    "print(sgd_clf.predict(x_.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1054697  0.0578808  0.10781884 0.13407862 0.09722768 0.10026815\n",
      "  0.08649302 0.0578361  0.10698884 0.1459383 ]]\n",
      "[[0.  0.2 0.2 0.  0.  0.  0.  0.  0.2 0.4]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "x = [[0,1,0,1,0,0,0]]\n",
    "x_,_,_,_,_ = process_data(x=x, y=x, poly_features=poly_features, pca=pca, OPTIMUM_DIMENSION=OPTIMUM_DIMENSION)\n",
    "print(model.predict(x_.copy()))\n",
    "print(forest_clf.predict_proba(x_.copy()))\n",
    "print(knn_clf.predict_proba(x_.copy()))\n",
    "print(sgd_clf.predict(x_.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cases_predictions(all_cases_features):\n",
    "    preds_df = pd.DataFrame(columns=ALL_CASES_PREDICTIONS_COLUMS)\n",
    "    preds_df[\"TYPE\"] = None\n",
    "    \n",
    "    all_cases_features,_,_,_,_ = process_data(x=all_cases_features, y=all_cases_features, \n",
    "                                              poly_features=poly_features, pca=pca, \n",
    "                                              OPTIMUM_DIMENSION=OPTIMUM_DIMENSION)\n",
    "    \n",
    "    mlp_preds = model.predict(all_cases_features.copy())\n",
    "    mlp_preds = np.multiply(mlp_preds, 100)\n",
    "    mlp_df = pd.DataFrame(mlp_preds, columns=ALL_CASES_PREDICTIONS_COLUMS)\n",
    "    print(mlp_preds)\n",
    "    mlp_df[\"TYPE\"] = \"MLP\"\n",
    "    preds_df = preds_df.append(mlp_df)\n",
    "    \n",
    "    rf_preds = forest_clf.predict(all_cases_features.copy())\n",
    "    knn_preds = knn_clf.predict(all_cases_features.copy())\n",
    "    sgd_preds = sgd_clf.predict(all_cases_features.copy())\n",
    "    \n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5532231 39.33496    6.9147215 ...  9.024629  14.086148   8.52484  ]\n",
      " [ 7.5096054 12.28265   13.228414  ...  8.306694   9.907604   6.382473 ]\n",
      " [ 3.334197  11.879541   8.093478  ...  7.020385   6.4445457  5.5389566]\n",
      " ...\n",
      " [10.774941   4.0829086 18.309965  ...  4.056551  13.190492   6.9754896]\n",
      " [58.334465   2.5842335  4.2555194 ...  3.2960277  9.138535   3.03973  ]\n",
      " [ 8.517955   5.9594374  7.3085656 ...  5.555501  45.96287    5.274289 ]]\n"
     ]
    }
   ],
   "source": [
    "all_cases_predictions_df = get_all_cases_predictions(all_cases_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.553223</td>\n",
       "      <td>39.334961</td>\n",
       "      <td>6.914721</td>\n",
       "      <td>4.262697</td>\n",
       "      <td>6.889342</td>\n",
       "      <td>3.366898</td>\n",
       "      <td>6.042534</td>\n",
       "      <td>9.024629</td>\n",
       "      <td>14.086148</td>\n",
       "      <td>8.524840</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.509605</td>\n",
       "      <td>12.282650</td>\n",
       "      <td>13.228414</td>\n",
       "      <td>9.628992</td>\n",
       "      <td>18.349697</td>\n",
       "      <td>8.372885</td>\n",
       "      <td>6.030983</td>\n",
       "      <td>8.306694</td>\n",
       "      <td>9.907604</td>\n",
       "      <td>6.382473</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.334197</td>\n",
       "      <td>11.879541</td>\n",
       "      <td>8.093478</td>\n",
       "      <td>9.054167</td>\n",
       "      <td>38.879452</td>\n",
       "      <td>3.876256</td>\n",
       "      <td>5.879017</td>\n",
       "      <td>7.020385</td>\n",
       "      <td>6.444546</td>\n",
       "      <td>5.538957</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.830506</td>\n",
       "      <td>7.526378</td>\n",
       "      <td>4.747234</td>\n",
       "      <td>10.053899</td>\n",
       "      <td>44.879513</td>\n",
       "      <td>6.374301</td>\n",
       "      <td>5.093182</td>\n",
       "      <td>5.423261</td>\n",
       "      <td>6.215277</td>\n",
       "      <td>4.856456</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.196395</td>\n",
       "      <td>13.264624</td>\n",
       "      <td>6.939872</td>\n",
       "      <td>4.464529</td>\n",
       "      <td>6.486824</td>\n",
       "      <td>6.161917</td>\n",
       "      <td>10.069639</td>\n",
       "      <td>7.112194</td>\n",
       "      <td>6.979593</td>\n",
       "      <td>34.324413</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.192058</td>\n",
       "      <td>7.471269</td>\n",
       "      <td>18.833324</td>\n",
       "      <td>7.330932</td>\n",
       "      <td>11.896738</td>\n",
       "      <td>7.181845</td>\n",
       "      <td>12.511088</td>\n",
       "      <td>7.406135</td>\n",
       "      <td>4.772666</td>\n",
       "      <td>15.403947</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.541879</td>\n",
       "      <td>6.164216</td>\n",
       "      <td>5.498583</td>\n",
       "      <td>15.079559</td>\n",
       "      <td>30.714527</td>\n",
       "      <td>4.132267</td>\n",
       "      <td>7.180537</td>\n",
       "      <td>6.662847</td>\n",
       "      <td>3.341108</td>\n",
       "      <td>12.684469</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.336843</td>\n",
       "      <td>7.559615</td>\n",
       "      <td>5.517073</td>\n",
       "      <td>13.722263</td>\n",
       "      <td>24.713366</td>\n",
       "      <td>8.548863</td>\n",
       "      <td>11.741745</td>\n",
       "      <td>6.569133</td>\n",
       "      <td>4.707669</td>\n",
       "      <td>10.583437</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.267414</td>\n",
       "      <td>16.135756</td>\n",
       "      <td>13.460493</td>\n",
       "      <td>7.860033</td>\n",
       "      <td>6.466064</td>\n",
       "      <td>9.541339</td>\n",
       "      <td>8.281985</td>\n",
       "      <td>10.077622</td>\n",
       "      <td>11.444051</td>\n",
       "      <td>10.465247</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.336064</td>\n",
       "      <td>5.892505</td>\n",
       "      <td>23.692274</td>\n",
       "      <td>12.671237</td>\n",
       "      <td>6.915795</td>\n",
       "      <td>12.022660</td>\n",
       "      <td>7.145582</td>\n",
       "      <td>7.949306</td>\n",
       "      <td>6.817841</td>\n",
       "      <td>7.556728</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.542850</td>\n",
       "      <td>8.172011</td>\n",
       "      <td>11.384916</td>\n",
       "      <td>9.412456</td>\n",
       "      <td>14.622158</td>\n",
       "      <td>10.376451</td>\n",
       "      <td>7.729083</td>\n",
       "      <td>6.975229</td>\n",
       "      <td>8.417619</td>\n",
       "      <td>11.367228</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.220421</td>\n",
       "      <td>6.439025</td>\n",
       "      <td>8.997533</td>\n",
       "      <td>9.551017</td>\n",
       "      <td>10.147505</td>\n",
       "      <td>17.411613</td>\n",
       "      <td>12.340570</td>\n",
       "      <td>7.396217</td>\n",
       "      <td>6.879205</td>\n",
       "      <td>11.616896</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.723465</td>\n",
       "      <td>6.444261</td>\n",
       "      <td>16.846584</td>\n",
       "      <td>5.617194</td>\n",
       "      <td>8.904403</td>\n",
       "      <td>5.707289</td>\n",
       "      <td>9.836530</td>\n",
       "      <td>8.617674</td>\n",
       "      <td>3.882613</td>\n",
       "      <td>21.419985</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.125234</td>\n",
       "      <td>3.400927</td>\n",
       "      <td>45.686680</td>\n",
       "      <td>7.801488</td>\n",
       "      <td>5.670408</td>\n",
       "      <td>4.360641</td>\n",
       "      <td>9.113519</td>\n",
       "      <td>7.935557</td>\n",
       "      <td>2.650771</td>\n",
       "      <td>7.254770</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>27.168417</td>\n",
       "      <td>5.192119</td>\n",
       "      <td>6.245614</td>\n",
       "      <td>9.108858</td>\n",
       "      <td>13.567047</td>\n",
       "      <td>4.806308</td>\n",
       "      <td>9.973742</td>\n",
       "      <td>7.286967</td>\n",
       "      <td>4.171725</td>\n",
       "      <td>12.479204</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.434368</td>\n",
       "      <td>5.403138</td>\n",
       "      <td>6.919079</td>\n",
       "      <td>5.893281</td>\n",
       "      <td>7.730361</td>\n",
       "      <td>6.470235</td>\n",
       "      <td>27.558331</td>\n",
       "      <td>11.865845</td>\n",
       "      <td>6.395267</td>\n",
       "      <td>9.330093</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.047108</td>\n",
       "      <td>55.071648</td>\n",
       "      <td>5.804783</td>\n",
       "      <td>6.204485</td>\n",
       "      <td>8.253347</td>\n",
       "      <td>1.883614</td>\n",
       "      <td>5.454927</td>\n",
       "      <td>9.758566</td>\n",
       "      <td>4.679670</td>\n",
       "      <td>1.841859</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.878354</td>\n",
       "      <td>17.238192</td>\n",
       "      <td>11.413236</td>\n",
       "      <td>9.818634</td>\n",
       "      <td>22.666641</td>\n",
       "      <td>6.254408</td>\n",
       "      <td>4.849634</td>\n",
       "      <td>6.643448</td>\n",
       "      <td>6.416408</td>\n",
       "      <td>3.821037</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.858342</td>\n",
       "      <td>12.367320</td>\n",
       "      <td>12.167258</td>\n",
       "      <td>8.149105</td>\n",
       "      <td>48.935528</td>\n",
       "      <td>2.368299</td>\n",
       "      <td>4.229717</td>\n",
       "      <td>6.287051</td>\n",
       "      <td>2.138052</td>\n",
       "      <td>1.499325</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.599021</td>\n",
       "      <td>7.255278</td>\n",
       "      <td>6.148334</td>\n",
       "      <td>7.154451</td>\n",
       "      <td>49.044861</td>\n",
       "      <td>6.218117</td>\n",
       "      <td>4.896777</td>\n",
       "      <td>4.099865</td>\n",
       "      <td>4.637453</td>\n",
       "      <td>2.945853</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.757053</td>\n",
       "      <td>24.159451</td>\n",
       "      <td>10.493711</td>\n",
       "      <td>6.944439</td>\n",
       "      <td>7.944014</td>\n",
       "      <td>6.380094</td>\n",
       "      <td>11.965944</td>\n",
       "      <td>10.527034</td>\n",
       "      <td>4.920029</td>\n",
       "      <td>12.908226</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.877368</td>\n",
       "      <td>11.167678</td>\n",
       "      <td>12.850057</td>\n",
       "      <td>7.690183</td>\n",
       "      <td>18.197855</td>\n",
       "      <td>7.142544</td>\n",
       "      <td>12.918364</td>\n",
       "      <td>7.160319</td>\n",
       "      <td>3.543330</td>\n",
       "      <td>12.452300</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.298155</td>\n",
       "      <td>11.547891</td>\n",
       "      <td>16.108614</td>\n",
       "      <td>11.319094</td>\n",
       "      <td>26.348236</td>\n",
       "      <td>4.943653</td>\n",
       "      <td>8.889363</td>\n",
       "      <td>8.848481</td>\n",
       "      <td>2.049069</td>\n",
       "      <td>4.647442</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.233870</td>\n",
       "      <td>11.485835</td>\n",
       "      <td>7.099364</td>\n",
       "      <td>8.750209</td>\n",
       "      <td>26.461283</td>\n",
       "      <td>6.120478</td>\n",
       "      <td>11.024381</td>\n",
       "      <td>5.681609</td>\n",
       "      <td>5.100750</td>\n",
       "      <td>10.042219</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.829028</td>\n",
       "      <td>19.518295</td>\n",
       "      <td>13.602885</td>\n",
       "      <td>13.100119</td>\n",
       "      <td>6.579478</td>\n",
       "      <td>9.224319</td>\n",
       "      <td>8.341658</td>\n",
       "      <td>11.593422</td>\n",
       "      <td>8.194061</td>\n",
       "      <td>5.016734</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.949741</td>\n",
       "      <td>9.881509</td>\n",
       "      <td>12.364678</td>\n",
       "      <td>21.268501</td>\n",
       "      <td>7.473948</td>\n",
       "      <td>12.807820</td>\n",
       "      <td>5.733393</td>\n",
       "      <td>7.852153</td>\n",
       "      <td>5.371872</td>\n",
       "      <td>7.296379</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.437007</td>\n",
       "      <td>7.289313</td>\n",
       "      <td>22.167244</td>\n",
       "      <td>10.674126</td>\n",
       "      <td>14.180244</td>\n",
       "      <td>10.678523</td>\n",
       "      <td>7.515112</td>\n",
       "      <td>5.573506</td>\n",
       "      <td>5.311604</td>\n",
       "      <td>6.173320</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.182670</td>\n",
       "      <td>8.060501</td>\n",
       "      <td>9.262791</td>\n",
       "      <td>8.403986</td>\n",
       "      <td>9.760775</td>\n",
       "      <td>21.349201</td>\n",
       "      <td>10.203230</td>\n",
       "      <td>5.788152</td>\n",
       "      <td>6.499898</td>\n",
       "      <td>11.488791</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.671268</td>\n",
       "      <td>7.874794</td>\n",
       "      <td>16.776707</td>\n",
       "      <td>5.385762</td>\n",
       "      <td>6.863308</td>\n",
       "      <td>10.542915</td>\n",
       "      <td>13.989507</td>\n",
       "      <td>10.272815</td>\n",
       "      <td>5.120453</td>\n",
       "      <td>11.502479</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.944290</td>\n",
       "      <td>6.013651</td>\n",
       "      <td>15.995126</td>\n",
       "      <td>9.744460</td>\n",
       "      <td>7.793327</td>\n",
       "      <td>7.544936</td>\n",
       "      <td>18.458860</td>\n",
       "      <td>12.195515</td>\n",
       "      <td>3.877025</td>\n",
       "      <td>10.432805</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10.450101</td>\n",
       "      <td>8.565024</td>\n",
       "      <td>5.794045</td>\n",
       "      <td>8.593863</td>\n",
       "      <td>6.467030</td>\n",
       "      <td>7.018950</td>\n",
       "      <td>7.831508</td>\n",
       "      <td>30.924202</td>\n",
       "      <td>4.692998</td>\n",
       "      <td>9.662278</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>7.575533</td>\n",
       "      <td>6.045641</td>\n",
       "      <td>8.429304</td>\n",
       "      <td>9.969258</td>\n",
       "      <td>5.389151</td>\n",
       "      <td>9.154380</td>\n",
       "      <td>10.645333</td>\n",
       "      <td>9.324007</td>\n",
       "      <td>9.173191</td>\n",
       "      <td>24.294195</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.228086</td>\n",
       "      <td>9.241343</td>\n",
       "      <td>6.089934</td>\n",
       "      <td>10.431414</td>\n",
       "      <td>5.193062</td>\n",
       "      <td>5.284709</td>\n",
       "      <td>5.076188</td>\n",
       "      <td>31.996166</td>\n",
       "      <td>5.304563</td>\n",
       "      <td>15.154531</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>4.671368</td>\n",
       "      <td>5.138827</td>\n",
       "      <td>30.105162</td>\n",
       "      <td>12.986943</td>\n",
       "      <td>4.172295</td>\n",
       "      <td>6.166340</td>\n",
       "      <td>5.693996</td>\n",
       "      <td>9.030559</td>\n",
       "      <td>8.632145</td>\n",
       "      <td>13.402359</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>32.092831</td>\n",
       "      <td>5.616095</td>\n",
       "      <td>4.243487</td>\n",
       "      <td>15.677229</td>\n",
       "      <td>7.228404</td>\n",
       "      <td>4.875949</td>\n",
       "      <td>4.457174</td>\n",
       "      <td>13.068313</td>\n",
       "      <td>6.209836</td>\n",
       "      <td>6.530694</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>9.466213</td>\n",
       "      <td>6.530720</td>\n",
       "      <td>9.827697</td>\n",
       "      <td>12.563363</td>\n",
       "      <td>5.279297</td>\n",
       "      <td>8.389819</td>\n",
       "      <td>6.914268</td>\n",
       "      <td>8.380885</td>\n",
       "      <td>21.931932</td>\n",
       "      <td>10.715811</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5.490243</td>\n",
       "      <td>10.213663</td>\n",
       "      <td>9.039139</td>\n",
       "      <td>21.229248</td>\n",
       "      <td>5.404862</td>\n",
       "      <td>7.190260</td>\n",
       "      <td>6.601893</td>\n",
       "      <td>15.953673</td>\n",
       "      <td>7.520570</td>\n",
       "      <td>11.356450</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3.454418</td>\n",
       "      <td>4.235618</td>\n",
       "      <td>20.855801</td>\n",
       "      <td>36.885330</td>\n",
       "      <td>3.960310</td>\n",
       "      <td>4.865851</td>\n",
       "      <td>5.279881</td>\n",
       "      <td>4.636766</td>\n",
       "      <td>5.257350</td>\n",
       "      <td>10.568668</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>18.326620</td>\n",
       "      <td>5.945343</td>\n",
       "      <td>7.204892</td>\n",
       "      <td>9.622448</td>\n",
       "      <td>7.126400</td>\n",
       "      <td>6.471156</td>\n",
       "      <td>10.698698</td>\n",
       "      <td>10.303958</td>\n",
       "      <td>6.642515</td>\n",
       "      <td>17.657970</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>5.421726</td>\n",
       "      <td>4.252885</td>\n",
       "      <td>9.647815</td>\n",
       "      <td>8.220371</td>\n",
       "      <td>3.934496</td>\n",
       "      <td>6.239606</td>\n",
       "      <td>9.611146</td>\n",
       "      <td>5.410928</td>\n",
       "      <td>8.547517</td>\n",
       "      <td>38.713512</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>16.703463</td>\n",
       "      <td>7.463532</td>\n",
       "      <td>16.631424</td>\n",
       "      <td>12.160860</td>\n",
       "      <td>10.156498</td>\n",
       "      <td>6.339450</td>\n",
       "      <td>5.269057</td>\n",
       "      <td>9.994332</td>\n",
       "      <td>8.002927</td>\n",
       "      <td>7.278453</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>3.759735</td>\n",
       "      <td>2.873645</td>\n",
       "      <td>57.132416</td>\n",
       "      <td>11.348257</td>\n",
       "      <td>4.490664</td>\n",
       "      <td>3.397369</td>\n",
       "      <td>3.650149</td>\n",
       "      <td>3.416397</td>\n",
       "      <td>5.728827</td>\n",
       "      <td>4.202543</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>62.629551</td>\n",
       "      <td>3.343482</td>\n",
       "      <td>3.769727</td>\n",
       "      <td>4.015837</td>\n",
       "      <td>5.129719</td>\n",
       "      <td>3.100262</td>\n",
       "      <td>3.400472</td>\n",
       "      <td>4.056645</td>\n",
       "      <td>7.341190</td>\n",
       "      <td>3.213116</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>7.723270</td>\n",
       "      <td>7.487333</td>\n",
       "      <td>11.328348</td>\n",
       "      <td>6.775012</td>\n",
       "      <td>3.426826</td>\n",
       "      <td>4.493097</td>\n",
       "      <td>7.461079</td>\n",
       "      <td>6.080037</td>\n",
       "      <td>39.317902</td>\n",
       "      <td>5.907100</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2.858170</td>\n",
       "      <td>8.028877</td>\n",
       "      <td>3.874197</td>\n",
       "      <td>3.174693</td>\n",
       "      <td>2.931898</td>\n",
       "      <td>2.667173</td>\n",
       "      <td>10.347362</td>\n",
       "      <td>60.453720</td>\n",
       "      <td>3.257887</td>\n",
       "      <td>2.406018</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>6.713665</td>\n",
       "      <td>8.311197</td>\n",
       "      <td>5.864713</td>\n",
       "      <td>22.637230</td>\n",
       "      <td>9.622909</td>\n",
       "      <td>8.228859</td>\n",
       "      <td>12.294590</td>\n",
       "      <td>11.738908</td>\n",
       "      <td>6.675042</td>\n",
       "      <td>7.912885</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>10.243517</td>\n",
       "      <td>7.468194</td>\n",
       "      <td>11.160149</td>\n",
       "      <td>6.538281</td>\n",
       "      <td>11.918929</td>\n",
       "      <td>5.780048</td>\n",
       "      <td>7.477196</td>\n",
       "      <td>28.499136</td>\n",
       "      <td>5.181542</td>\n",
       "      <td>5.733010</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.777427</td>\n",
       "      <td>7.431024</td>\n",
       "      <td>7.379228</td>\n",
       "      <td>9.964579</td>\n",
       "      <td>15.134950</td>\n",
       "      <td>9.481400</td>\n",
       "      <td>10.769507</td>\n",
       "      <td>7.526424</td>\n",
       "      <td>7.222219</td>\n",
       "      <td>18.313242</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>7.553872</td>\n",
       "      <td>9.813243</td>\n",
       "      <td>11.986659</td>\n",
       "      <td>8.546484</td>\n",
       "      <td>5.807601</td>\n",
       "      <td>9.619576</td>\n",
       "      <td>9.113855</td>\n",
       "      <td>23.384340</td>\n",
       "      <td>7.892928</td>\n",
       "      <td>6.281438</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>7.696512</td>\n",
       "      <td>7.017382</td>\n",
       "      <td>15.955670</td>\n",
       "      <td>14.165733</td>\n",
       "      <td>7.178235</td>\n",
       "      <td>12.415036</td>\n",
       "      <td>8.482285</td>\n",
       "      <td>7.393743</td>\n",
       "      <td>11.143192</td>\n",
       "      <td>8.552208</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>23.459301</td>\n",
       "      <td>5.499379</td>\n",
       "      <td>12.005052</td>\n",
       "      <td>12.571282</td>\n",
       "      <td>10.579813</td>\n",
       "      <td>7.380401</td>\n",
       "      <td>5.141896</td>\n",
       "      <td>11.406551</td>\n",
       "      <td>7.653967</td>\n",
       "      <td>4.302351</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>9.176565</td>\n",
       "      <td>8.237069</td>\n",
       "      <td>10.169098</td>\n",
       "      <td>10.549981</td>\n",
       "      <td>7.753926</td>\n",
       "      <td>10.453171</td>\n",
       "      <td>8.257619</td>\n",
       "      <td>5.296042</td>\n",
       "      <td>22.805405</td>\n",
       "      <td>7.301124</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>7.109716</td>\n",
       "      <td>7.829186</td>\n",
       "      <td>5.947896</td>\n",
       "      <td>25.440025</td>\n",
       "      <td>9.084385</td>\n",
       "      <td>9.665501</td>\n",
       "      <td>11.161074</td>\n",
       "      <td>8.797921</td>\n",
       "      <td>7.451574</td>\n",
       "      <td>7.512724</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4.713635</td>\n",
       "      <td>2.884311</td>\n",
       "      <td>5.083187</td>\n",
       "      <td>52.718918</td>\n",
       "      <td>7.234634</td>\n",
       "      <td>6.383015</td>\n",
       "      <td>5.848530</td>\n",
       "      <td>3.330695</td>\n",
       "      <td>3.304263</td>\n",
       "      <td>8.498801</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>23.516073</td>\n",
       "      <td>4.745094</td>\n",
       "      <td>8.037331</td>\n",
       "      <td>8.396552</td>\n",
       "      <td>14.174252</td>\n",
       "      <td>8.903766</td>\n",
       "      <td>6.603445</td>\n",
       "      <td>7.236863</td>\n",
       "      <td>6.454324</td>\n",
       "      <td>11.932300</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>7.410928</td>\n",
       "      <td>3.617790</td>\n",
       "      <td>5.829947</td>\n",
       "      <td>7.515857</td>\n",
       "      <td>8.041281</td>\n",
       "      <td>8.707794</td>\n",
       "      <td>10.410825</td>\n",
       "      <td>4.183305</td>\n",
       "      <td>9.546854</td>\n",
       "      <td>34.735420</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>22.140577</td>\n",
       "      <td>5.589299</td>\n",
       "      <td>10.570569</td>\n",
       "      <td>13.071004</td>\n",
       "      <td>11.434548</td>\n",
       "      <td>9.648956</td>\n",
       "      <td>5.710658</td>\n",
       "      <td>6.302813</td>\n",
       "      <td>9.790574</td>\n",
       "      <td>5.740992</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>10.774941</td>\n",
       "      <td>4.082909</td>\n",
       "      <td>18.309965</td>\n",
       "      <td>20.199520</td>\n",
       "      <td>8.928364</td>\n",
       "      <td>7.110230</td>\n",
       "      <td>6.371537</td>\n",
       "      <td>4.056551</td>\n",
       "      <td>13.190492</td>\n",
       "      <td>6.975490</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>58.334465</td>\n",
       "      <td>2.584234</td>\n",
       "      <td>4.255519</td>\n",
       "      <td>4.792693</td>\n",
       "      <td>7.937356</td>\n",
       "      <td>4.145945</td>\n",
       "      <td>2.475496</td>\n",
       "      <td>3.296028</td>\n",
       "      <td>9.138535</td>\n",
       "      <td>3.039730</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>8.517955</td>\n",
       "      <td>5.959437</td>\n",
       "      <td>7.308566</td>\n",
       "      <td>5.438159</td>\n",
       "      <td>4.512428</td>\n",
       "      <td>5.058444</td>\n",
       "      <td>6.412349</td>\n",
       "      <td>5.555501</td>\n",
       "      <td>45.962872</td>\n",
       "      <td>5.274289</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5  \\\n",
       "0     1.553223  39.334961   6.914721   4.262697   6.889342   3.366898   \n",
       "1     7.509605  12.282650  13.228414   9.628992  18.349697   8.372885   \n",
       "2     3.334197  11.879541   8.093478   9.054167  38.879452   3.876256   \n",
       "3     4.830506   7.526378   4.747234  10.053899  44.879513   6.374301   \n",
       "4     4.196395  13.264624   6.939872   4.464529   6.486824   6.161917   \n",
       "5     7.192058   7.471269  18.833324   7.330932  11.896738   7.181845   \n",
       "6     8.541879   6.164216   5.498583  15.079559  30.714527   4.132267   \n",
       "7     6.336843   7.559615   5.517073  13.722263  24.713366   8.548863   \n",
       "8     6.267414  16.135756  13.460493   7.860033   6.466064   9.541339   \n",
       "9     9.336064   5.892505  23.692274  12.671237   6.915795  12.022660   \n",
       "10   11.542850   8.172011  11.384916   9.412456  14.622158  10.376451   \n",
       "11    9.220421   6.439025   8.997533   9.551017  10.147505  17.411613   \n",
       "12   12.723465   6.444261  16.846584   5.617194   8.904403   5.707289   \n",
       "13    6.125234   3.400927  45.686680   7.801488   5.670408   4.360641   \n",
       "14   27.168417   5.192119   6.245614   9.108858  13.567047   4.806308   \n",
       "15   12.434368   5.403138   6.919079   5.893281   7.730361   6.470235   \n",
       "16    1.047108  55.071648   5.804783   6.204485   8.253347   1.883614   \n",
       "17   10.878354  17.238192  11.413236   9.818634  22.666641   6.254408   \n",
       "18    1.858342  12.367320  12.167258   8.149105  48.935528   2.368299   \n",
       "19    7.599021   7.255278   6.148334   7.154451  49.044861   6.218117   \n",
       "20    3.757053  24.159451  10.493711   6.944439   7.944014   6.380094   \n",
       "21    6.877368  11.167678  12.850057   7.690183  18.197855   7.142544   \n",
       "22    5.298155  11.547891  16.108614  11.319094  26.348236   4.943653   \n",
       "23    8.233870  11.485835   7.099364   8.750209  26.461283   6.120478   \n",
       "24    4.829028  19.518295  13.602885  13.100119   6.579478   9.224319   \n",
       "25    9.949741   9.881509  12.364678  21.268501   7.473948  12.807820   \n",
       "26   10.437007   7.289313  22.167244  10.674126  14.180244  10.678523   \n",
       "27    9.182670   8.060501   9.262791   8.403986   9.760775  21.349201   \n",
       "28   11.671268   7.874794  16.776707   5.385762   6.863308  10.542915   \n",
       "29    7.944290   6.013651  15.995126   9.744460   7.793327   7.544936   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "98   10.450101   8.565024   5.794045   8.593863   6.467030   7.018950   \n",
       "99    7.575533   6.045641   8.429304   9.969258   5.389151   9.154380   \n",
       "100   6.228086   9.241343   6.089934  10.431414   5.193062   5.284709   \n",
       "101   4.671368   5.138827  30.105162  12.986943   4.172295   6.166340   \n",
       "102  32.092831   5.616095   4.243487  15.677229   7.228404   4.875949   \n",
       "103   9.466213   6.530720   9.827697  12.563363   5.279297   8.389819   \n",
       "104   5.490243  10.213663   9.039139  21.229248   5.404862   7.190260   \n",
       "105   3.454418   4.235618  20.855801  36.885330   3.960310   4.865851   \n",
       "106  18.326620   5.945343   7.204892   9.622448   7.126400   6.471156   \n",
       "107   5.421726   4.252885   9.647815   8.220371   3.934496   6.239606   \n",
       "108  16.703463   7.463532  16.631424  12.160860  10.156498   6.339450   \n",
       "109   3.759735   2.873645  57.132416  11.348257   4.490664   3.397369   \n",
       "110  62.629551   3.343482   3.769727   4.015837   5.129719   3.100262   \n",
       "111   7.723270   7.487333  11.328348   6.775012   3.426826   4.493097   \n",
       "112   2.858170   8.028877   3.874197   3.174693   2.931898   2.667173   \n",
       "113   6.713665   8.311197   5.864713  22.637230   9.622909   8.228859   \n",
       "114  10.243517   7.468194  11.160149   6.538281  11.918929   5.780048   \n",
       "115   6.777427   7.431024   7.379228   9.964579  15.134950   9.481400   \n",
       "116   7.553872   9.813243  11.986659   8.546484   5.807601   9.619576   \n",
       "117   7.696512   7.017382  15.955670  14.165733   7.178235  12.415036   \n",
       "118  23.459301   5.499379  12.005052  12.571282  10.579813   7.380401   \n",
       "119   9.176565   8.237069  10.169098  10.549981   7.753926  10.453171   \n",
       "120   7.109716   7.829186   5.947896  25.440025   9.084385   9.665501   \n",
       "121   4.713635   2.884311   5.083187  52.718918   7.234634   6.383015   \n",
       "122  23.516073   4.745094   8.037331   8.396552  14.174252   8.903766   \n",
       "123   7.410928   3.617790   5.829947   7.515857   8.041281   8.707794   \n",
       "124  22.140577   5.589299  10.570569  13.071004  11.434548   9.648956   \n",
       "125  10.774941   4.082909  18.309965  20.199520   8.928364   7.110230   \n",
       "126  58.334465   2.584234   4.255519   4.792693   7.937356   4.145945   \n",
       "127   8.517955   5.959437   7.308566   5.438159   4.512428   5.058444   \n",
       "\n",
       "             6          7          8          9 TYPE  \n",
       "0     6.042534   9.024629  14.086148   8.524840  MLP  \n",
       "1     6.030983   8.306694   9.907604   6.382473  MLP  \n",
       "2     5.879017   7.020385   6.444546   5.538957  MLP  \n",
       "3     5.093182   5.423261   6.215277   4.856456  MLP  \n",
       "4    10.069639   7.112194   6.979593  34.324413  MLP  \n",
       "5    12.511088   7.406135   4.772666  15.403947  MLP  \n",
       "6     7.180537   6.662847   3.341108  12.684469  MLP  \n",
       "7    11.741745   6.569133   4.707669  10.583437  MLP  \n",
       "8     8.281985  10.077622  11.444051  10.465247  MLP  \n",
       "9     7.145582   7.949306   6.817841   7.556728  MLP  \n",
       "10    7.729083   6.975229   8.417619  11.367228  MLP  \n",
       "11   12.340570   7.396217   6.879205  11.616896  MLP  \n",
       "12    9.836530   8.617674   3.882613  21.419985  MLP  \n",
       "13    9.113519   7.935557   2.650771   7.254770  MLP  \n",
       "14    9.973742   7.286967   4.171725  12.479204  MLP  \n",
       "15   27.558331  11.865845   6.395267   9.330093  MLP  \n",
       "16    5.454927   9.758566   4.679670   1.841859  MLP  \n",
       "17    4.849634   6.643448   6.416408   3.821037  MLP  \n",
       "18    4.229717   6.287051   2.138052   1.499325  MLP  \n",
       "19    4.896777   4.099865   4.637453   2.945853  MLP  \n",
       "20   11.965944  10.527034   4.920029  12.908226  MLP  \n",
       "21   12.918364   7.160319   3.543330  12.452300  MLP  \n",
       "22    8.889363   8.848481   2.049069   4.647442  MLP  \n",
       "23   11.024381   5.681609   5.100750  10.042219  MLP  \n",
       "24    8.341658  11.593422   8.194061   5.016734  MLP  \n",
       "25    5.733393   7.852153   5.371872   7.296379  MLP  \n",
       "26    7.515112   5.573506   5.311604   6.173320  MLP  \n",
       "27   10.203230   5.788152   6.499898  11.488791  MLP  \n",
       "28   13.989507  10.272815   5.120453  11.502479  MLP  \n",
       "29   18.458860  12.195515   3.877025  10.432805  MLP  \n",
       "..         ...        ...        ...        ...  ...  \n",
       "98    7.831508  30.924202   4.692998   9.662278  MLP  \n",
       "99   10.645333   9.324007   9.173191  24.294195  MLP  \n",
       "100   5.076188  31.996166   5.304563  15.154531  MLP  \n",
       "101   5.693996   9.030559   8.632145  13.402359  MLP  \n",
       "102   4.457174  13.068313   6.209836   6.530694  MLP  \n",
       "103   6.914268   8.380885  21.931932  10.715811  MLP  \n",
       "104   6.601893  15.953673   7.520570  11.356450  MLP  \n",
       "105   5.279881   4.636766   5.257350  10.568668  MLP  \n",
       "106  10.698698  10.303958   6.642515  17.657970  MLP  \n",
       "107   9.611146   5.410928   8.547517  38.713512  MLP  \n",
       "108   5.269057   9.994332   8.002927   7.278453  MLP  \n",
       "109   3.650149   3.416397   5.728827   4.202543  MLP  \n",
       "110   3.400472   4.056645   7.341190   3.213116  MLP  \n",
       "111   7.461079   6.080037  39.317902   5.907100  MLP  \n",
       "112  10.347362  60.453720   3.257887   2.406018  MLP  \n",
       "113  12.294590  11.738908   6.675042   7.912885  MLP  \n",
       "114   7.477196  28.499136   5.181542   5.733010  MLP  \n",
       "115  10.769507   7.526424   7.222219  18.313242  MLP  \n",
       "116   9.113855  23.384340   7.892928   6.281438  MLP  \n",
       "117   8.482285   7.393743  11.143192   8.552208  MLP  \n",
       "118   5.141896  11.406551   7.653967   4.302351  MLP  \n",
       "119   8.257619   5.296042  22.805405   7.301124  MLP  \n",
       "120  11.161074   8.797921   7.451574   7.512724  MLP  \n",
       "121   5.848530   3.330695   3.304263   8.498801  MLP  \n",
       "122   6.603445   7.236863   6.454324  11.932300  MLP  \n",
       "123  10.410825   4.183305   9.546854  34.735420  MLP  \n",
       "124   5.710658   6.302813   9.790574   5.740992  MLP  \n",
       "125   6.371537   4.056551  13.190492   6.975490  MLP  \n",
       "126   2.475496   3.296028   9.138535   3.039730  MLP  \n",
       "127   6.412349   5.555501  45.962872   5.274289  MLP  \n",
       "\n",
       "[128 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cases_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
